<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project page of the paper, Target-Aware Video Diffusion Models">
  <meta property="og:title" content="Target-Aware Video Diffusion Models"/>
  <meta property="og:description" content="Given an input image, our target-aware video diffusion model generates a video in which an actor accurately interacts with a specified target. The target is indicated to the model via a segmentation mask, while the desired action is described with a text prompt."/>
  <meta property="og:url" content="https://taeksuu.github.io/tavid"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/banner_resized.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="450"/>


  <meta name="twitter:title" content="Target-Aware Video Diffusion Models">
  <meta name="twitter:description" content="Given an input image, our target-aware video diffusion model generates a video in which an actor accurately interacts with a specified target. The target is indicated to the model via a segmentation mask, while the desired action is described with a text prompt.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner_resized.png">
  <meta name="twitter:card" content="Target-Aware Video Diffusion Models">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Image-to-video, Controllable video generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Target-Aware Video Diffusion Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Target-Aware Video Diffusion Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://taeksuu.github.io/" target="_blank">Taeksoo Kim<sup>1</sup></a>,&nbsp</span>
                <span class="author-block">
                  <a href="https://jhugestar.github.io/" target="_blank">Hanbyul Joo<sup>1,2</sup></a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Seoul National University,&nbsp</span><span class="author-block"><sup>2</sup>RLWRLD</span><br>ICLR 2026</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2503.18950.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/taeksuu/tavid" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code & Data </span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.18950" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="subtitle has-text-centered">
      <b>TL; DR.</b> Our target-aware model generates a video in which<br>an actor accurately interacts with the target, specified with its segmentation mask.
    </h2>
    <div class="hero-body has-text-centered">
      <video id="teaser" autoplay muted loop playsinline width="60%">
        <!-- Your video here -->
        <source src="static/videos/tavid_teaser.mp4" type="video/mp4">
      </video>
      <!-- <style>
        #teaser {
          clip-path: inset(2px 2px 2.5px 2px); /* Crop 2px from each side */
        }
      </style> -->
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present a target-aware video diffusion model that generates videos from an input image, in which an actor interacts with a specified target while performing a desired action. The target is defined by a segmentation mask, and the action is described through a text prompt. <br><br>
            Our key motivation is to incorporate target awareness into video generation, enabling actors to perform directed actions on designated objects. This enables video diffusion models to act as motion planners, producing plausible predictions of human-object interactions by leveraging the priors of large-scale video generative models. <br><br>
            We build our target-aware model by extending a baseline model to incorporate the target mask as an additional input. To enforce target awareness, we introduce a special token that encodes the target's spatial information within the text prompt. We then fine-tune the model with our curated dataset using an additional cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask. To further improve performance, we selectively apply this loss to the most semantically relevant attention regions and transformer blocks. <br><br>
            Experimental results show that our target-aware model outperforms existing solutions in generating videos where actors interact accurately with the specified targets. We further demonstrate its efficacy in two downstream applications: zero-shot 3D HOI motion synthesis with physical plausibility and long-term video content creation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Results -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 is-centered has-text-centered">Results</h2>

    <h2 class="title is-5">Comparison on target alignment</h2>
    <p>Our target-aware model generates videos where the actor interacts with the specified target. Baseline methods often hallucinate the target described in the input text prompt, leading to unintended outputs.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="ta" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/ta_teddy.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            "The girl turns and picks up the <span style="color:green">[TGT]</span> teddy bear resting on the bed."
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="ta" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/ta_coke.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            "The man lifts the <span style="color:green">[TGT]</span> bottle of coke and takes a slow sip."
          </p>
        </div>
      </div>
    </div>
  
    <h2 class="title is-5">Multiple objects of the same type</h2>
    <p>In scenes with multiple objects of the same type, our method enables actors to accurately interact with the intended target by leveraging its mask.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="mo" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/multi_obj.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            "The woman picks up the <span style="color:green">[TGT]</span> mug cup and takes a sip of coffee."
          </p>
        </div>
      </div>
    </div>

    <h2 class="title is-5">Non-human interactions</h2>
    <p>While trained on human-scene interaction datasets, our target-aware model generalizes to non-human interactions.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content is-centered">
          <video id="nh" autoplay muted loop playsinline width="100%">
            <source src="./static/videos/non_hum.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            Left: “The rabbit turns its head towards the <span style="color:green">[TGT]</span> carrot and takes a bite of it.” <br>
            Right: “The dog bites onto the <span style="color:green">[TGT]</span> frisbee and lifts it off the ground.”
          </p>
        </div>
      </div>
    </div>

    <h2 class="title is-5">Control over both the actor and the target</h2>
    <p>Our model, extended to take in two masks, enables specifying both the source actor and the target object. The actor is indicated with a <span style="color:red">red</span> mask and the target is indicated with a <span style="color:green">green</span> mask.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="mo" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/two_masks.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            “The <span style="color:red">[SRC]</span> robotic arm picks up the  <span style="color:green">[TGT]</span> blue can with its robot hand.”
          </p>
        </div>
      </div>
    </div>

    <br>

    <!-- Toggle Button ABOVE the content -->
    <div class="has-text-centered">
      <button class="button is-link is-light" onclick="toggleResults()">Click for more results</button>
    </div>

    <!-- Hidden Section STARTS here -->
    <div id="results-section" style="display: none;">

      <h2 class="title is-5">Targeting in complex scenes</h2>
      <p>
        Our model successfully generates videos that capture accurate interactions with the target object, even when the target occupies only a small portion of a complex scene.
      </p>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <video autoplay muted loop playsinline height="100%">
              <source src="./static/videos/ego4d_1.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              “The man picks up the <span style="color:green">[TGT]</span> tumbler bottle from the table and takes a drink of water.” 
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <video autoplay muted loop playsinline height="100%">
              <source src="./static/videos/ego4d_2.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              “The woman picks up the <span style="color:green">[TGT]</span> red carton.”
            </p>
          </div>
        </div>
      </div>

      <h2 class="title is-5">Providing motion</h2>
      <p>
        Our output videos can serve as a source of motion data for existing controllable video generation approaches, such as 
        <a href="https://igl-hkust.github.io/das/" target="_blank">Diffusion as Shader (Das)</a>.
      </p>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <video autoplay muted loop playsinline height="100%">
              <source src="./static/videos/das_1.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              “The person sits on the <span style="color:green">[TGT]</span> chair.”
            </p>
          </div>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <video autoplay muted loop playsinline height="100%">
              <source src="./static/videos/das_2.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              “The rabbit turns its head towards the <span style="color:green">[TGT]</span> carrot and takes a bite.”
            </p>
          </div>
        </div>
      </div>

    </div>

    <!-- Toggle Script -->
    <script>
      function toggleResults() {
        const section = document.getElementById("results-section");
        section.style.display = section.style.display === "none" ? "block" : "none";
      }
    </script>


  </div>
</section>


<!-- Applications -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 is-centered has-text-centered">Applications</h2>

    <h2 class="title is-5">Video content creation</h2>
    <p>Given images of a person and a scene, we perform depth-based 3D insertion of the person
      into the scene and render them together to produce frames for video diffusion input. We interpolate generated initial and final frames to
      synthesize navigation contents, and utilize our target-aware video diffusion model to synthesize action and manipulation contents.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="vcc" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/vcc_crf.mp4" type="video/mp4">
          </video>
          <style>
            #vcc {
              clip-path: inset(2px 2px 2px 2px); /* Crop 2px from each side */
            }
          </style>
        </div>
      </div>
    </div>

  
    <h2 class="title is-5">Zero-shot 3D human-object interaction synthesis</h2>
    <p>We demonstrate our target-aware model's connection with robotic applications by performing imitation learning on 3D poses of a person interacting with a target in the scene.
      We use <a href="https://zju3dv.github.io/gvhmr/GVHMR" target="_blank">GVHMR</a> to estimate 3D human motions from our generated videos and <a href="https://wyhuai.github.io/physhoi-page/" target="_blank">PhysHOI</a> to perform physics-based imitation learning of the human motion.</p>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="mo" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/ir_2.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="mo" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/ir_1.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Method -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <br>
      <h2 class="title is-3 has-text-centered">Method</h2>
      <h3>
        1. We first extend a baseline video diffusion model to incorporate the mask as an additional input. <br>
        2. We introduce a <span style="color:green">[TGT]</span> token that will be used to encode target’s spatial information in the text prompt. <br>
        3. We fine-tune the model using our cross-attention loss that aligns the cross-attention maps associated with this token with the input target mask.
        <br><br>
      </h3>
      
      <div class="columns is-centered">
        <img src="static/images/method_0.png" alt="Random Image" id="tree" width="45%">
      </div>

      <h3>
        During inference, we prepend the <span style="color:green">[TGT]</span> token to words referring to the target and enable the model to leverage the spatial cue provided by the mask. Our cross-attention loss effectively guides the <span style="color:green">[TGT]</span> token to focus on the target region, enabling precise interactions between the actor and the target.
        <br><br>
      </h3>

      <div class="columns is-centered">
        <img src="static/images/method_1.png" alt="Random Image" id="tree" width="60%">
      </div>
      <br>
      <!-- Toggle Button -->
      <div class="has-text-centered">
        <button class="button is-link is-light" onclick="toggleDetails()">Click for details</button>
      </div>

      <!-- Hidden Section -->
      <div id="details-section" style="display: none;">
        <h3>
          <br>
          For effective and efficient supervision, we selectively apply our cross-attention loss to the model by identifying: <br>
          1. <b>transformer blocks that best capture semantic details</b> (Left) : We first generate 100 videos and compute L2 error between cross-attention map of each block and the segmentation mask for a predefined token to evaluate semantic alignment of each block. We apply our loss to region of blocks with the least error.<br>
          2. <b>cross-attention regions that most influence target awareness of the model</b> (Right) : While both text-to-video (T2V) and video-to-text (V2T) cross-attention maps encode semantic information, we apply our cross-attention loss on V2T cross-attention regions, since V2T attention directly influences the video latent representations.<br>
          <br>
        </h3>
        <div class="columns is-centered">
          <img src="static/images/method_2.png" alt="Random Image" id="tree" width="80%">
        </div>

        <h3>
          <br>
          To integrate the target mask during generation, we extend our base image-to-video diffusion model, <a href="https://github.com/THUDM/CogVideo" target="_blank">CogVideoX</a>. Specifically, we concatenate the downsampled mask alongside the input image condition and append an extra input channel to the original image projection layer. During fine-tuning, we train the image projection layer and the transformer.
          <br>
          <br>
        </h3>
        <div class="columns is-centered">
          <img src="static/images/method_3.png" alt="Random Image" id="tree" width="50%">
        </div>

        <h3>
          <br>
          To train our target-aware model, we curate 1290 videos from <a href="https://ego-exo4d-data.org/" target="_blank">Ego-Exo4D</a> and <a href="https://virtualhumans.mpi-inf.mpg.de/behave/" target="_blank">BEHAVE</a> datasets that satisfy two conditions: <br>
          1. the initial frame should depict a scene where an actor is present but not yet interacting with the target, and <br>
          2. subsequent frames must capture the actor engaging with the target. <br>
          We obtain the mask for the target in the initial frame using <a href="https://github.com/facebookresearch/segment-anything" target="_blank">SAM</a> and generate text prompts with <a href="https://huggingface.co/THUDM/cogvlm2-llama3-caption" target="_blank">CogVLM2-Caption</a>. While it is ideal to prepend <span style="color:green">[TGT]</span> tokens to the target object tokens during caption generation, we find that current video captioning tools cannot reliably identify the target object in complex scenes. Therefore, we add a general sentence, “The person interacts with <span style="color:green">[TGT]</span> object.” to the generated captions, which we find sufficient to train our target-aware model.

          <br>
          <br>
        </h3>
        <div class="content">
          <video id="method_4" autoplay muted loop playsinline height="100%">
            <source src="./static/videos/method_4_crf.mp4" type="video/mp4">
          </video>
        </div>
        
        <style>
          #method_4 {
            clip-path: inset(2px 2px 2.5px 2px); /* Crop 2px from each side */
          }
        </style>
      </div>


    </div>
  </div>
</section>

<!-- JavaScript for Toggle -->
<script>
  function toggleDetails() {
    var detailsSection = document.getElementById("details-section");
    if (detailsSection.style.display === "none") {
      detailsSection.style.display = "block";
    } else {
      detailsSection.style.display = "none";
    }
  }
</script>



<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{kim2025target,
        title={Target-Aware Video Diffusion Models},
        author={Kim, Taeksoo and Joo, Hanbyul},
        booktitle=iclr,
        year={2026}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
